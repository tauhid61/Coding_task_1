{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python389jvsc74a57bd0303f4f94c3b299c215f3066923e10ebd3683e42f3be4d2a62ff11651779c3d00",
   "display_name": "Python 3.8.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "303f4f94c3b299c215f3066923e10ebd3683e42f3be4d2a62ff11651779c3d00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "source": [
    "Given, <br>\n",
    "    nx = 2, nh = 4, ny = 1 <br>\n",
    "So, <br>\n",
    "    W1.shape == (nh, nx) == (4,2) <br>\n",
    "    b1.shape == (nh, 1) == (4,1) <br>\n",
    "    w2.shape == (ny, nh) == (1, 4) <br>\n",
    "    b1.shape == (ny, 1) == (1, 1)<br>\n",
    "    \n",
    "\n",
    "    "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Param initialization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(nx, nh, ny):\n",
    "    W1 = np.random.randn(nh, nx)*0.01\n",
    "    b1 = np.zeros((nh, 1))\n",
    "    W2 = np.random.randn(ny, nh) * 0.01\n",
    "    b2 = np.zeros((ny, 1))\n",
    "\n",
    "    assert(W1.shape == (nh, nx))\n",
    "    assert(b1.shape == (nh, 1))\n",
    "    assert(W2.shape == (ny, nh))\n",
    "    assert(b2.shape == (ny,1))\n",
    "    params = {'W1':W1, 'b1':b1, 'W2':W2, 'b2':b2}\n",
    "    return params"
   ]
  },
  {
   "source": [
    "## Forward Prop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1 / (1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z\n",
    "    assert(A.shape() == Z.shape())\n",
    "    return A, cache\n",
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    assert(Z.shape() == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache\n",
    "def linear_forward_activation(A_prev, W, b, activation):\n",
    "    if activation == 'sigmoid':\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == 'relu':\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    assert(A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "source": [
    "## Back Prop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_back(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1 / (1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    assert(dZ.shape == Z.shape)\n",
    "    return dZ\n",
    "def relu_back(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z<=0] = 0\n",
    "    assert(dZ.shape == Z.shape)\n",
    "    return dZ\n",
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache;\n",
    "    m = A_prev.shape[1];\n",
    "    dW = (1/m) * np.dot(dZ ,A_prev.T)\n",
    "    db = 1 / m * np.sum(dZ, axis = 1, keepdims = True) \n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.shape == b.shape)\n",
    "    return dA_prev, dW, db\n",
    "def linear_backward_activation(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation == 'sigmoid':\n",
    "        Z = sigmoid_back(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "source": [
    "## Cost function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1];\n",
    "    cost = -1 / m * (np.dot(Y, np.log(AL).T) + np.dot(1 - Y, np.log(1 - AL).T))\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    return cost"
   ]
  },
  {
   "source": [
    "## Update Params"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(params, grads, lr):\n",
    "    L = len(params) // 2\n",
    "    for l in range(L):\n",
    "        params['W'+str(l+1)] -= lr*grads['dW'+str(l+1)]\n",
    "        params['b'+str(l+1)] -= lr*grads['db'+str(l+1)]\n",
    "    return params"
   ]
  },
  {
   "source": [
    "# Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layer_dims, num_iter=500, lr=0.01):\n",
    "    grads={}\n",
    "    cost=[]\n",
    "    m = X.shape[1]\n",
    "    (nx, nh, ny) = layer_dims\n",
    "    params = init_params(nx, nh, ny)\n",
    "    W1, b1, W2, b2 = params['W1'], params['b1'], params['W2'], params['b2']\n",
    "    for i in range(0, num_iter):\n",
    "        A1, cache1 = linear_forward_activation(X, W1, b1, activation='relu')\n",
    "        A2, cache2 = linear_forward_activation(A1, W2, b2, activation = 'sigmoid')\n",
    "        cost = compute_cost(A2, Y)\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1-Y, 1-A2))\n",
    "        dA1, dW2, db2 = linear_backward_activation(dA2, cache2, activation='sigmoid')\n",
    "        dA0, dW1, db1 = linear_backward_activation(dA1, cache1, activation='relu')\n",
    "        grads['dW1'], grads['db1'], grads['dW2'], grads['db2'] = dW1, db1, dW2, db2\n",
    "        params = update_params(params, grads, lr)\n",
    "        W1, b1, W2, b2 = params['W1'], params['b1'], params['W2'], params['b2']\n",
    "        print(cost)\n",
    "    return params"
   ]
  }
 ]
}